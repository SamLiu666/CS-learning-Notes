{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.1常量张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量即多为数组， tensorflow的数据结构即为张量\n",
    "a = tf.constant(1) # tf.int32 类型常量\n",
    "b = tf.constant(1.23) #tf.float32 类型常量\n",
    "c = tf.constant(\"hello world\") # tf.string类型常量\n",
    "d=  tf.constant(True) #tf.bool类型常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(1.23, shape=(), dtype=float32)\n",
      "tf.Tensor(b'hello world', shape=(), dtype=string)\n",
      "tf.Tensor(True, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "print(a,b,c,d, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.rank(a))  # 0维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [] 中括号的数量代表张量的维度\n",
    "vector_1 = tf.constant([1.0,2.0,3.0,4.0]) #向量，1维张量\n",
    "vector_2 = tf.constant([[1,2],[3,4]])     # 2维张量\n",
    "vector_3 = tf.constant([[[1,2],[3,4]],[[1,2],[3,4]]])  # 3维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(2, shape=(), dtype=int32) tf.Tensor(3, shape=(), dtype=int32) \n",
      " (4,) (2, 2) (2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(tf.rank(vector_1), tf.rank(vector_2),tf.rank(vector_3),'\\n' ,vector_1.shape, vector_2.shape, vector_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2. 3. 4.], shape=(4,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[1 2]\n",
      "  [3 4]]], shape=(2, 2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(vector_1, vector_2, vector_3, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_3_float = tf.cast(vector_3, tf.float32)   # tf.cast() 改变张量的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 2.]\n",
      "  [3. 4.]]\n",
      "\n",
      " [[1. 2.]\n",
      "  [3. 4.]]], shape=(2, 2, 2), dtype=float32) \n",
      " tf.Tensor(\n",
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[1 2]\n",
      "  [3 4]]], shape=(2, 2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(v_3_float,'\\n',vector_3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_4= vector_2+tf.constant([[1.0,1.0],[1.0,1.0]])   # 不同整数类型无法相加\n",
    "vector_2 = tf.cast(vector_2, tf.float32)  \n",
    "vector_4= vector_2+tf.constant([[1.0,1.0],[1.0,1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2. 3.]\n",
      " [4. 5.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(vector_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.2模型中被训练的参数设置为变量 变量张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable([1.0,2.0],name = \"v\")\n",
    "b = tf.Variable([1.0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)>\n",
      "1626276329544\n",
      "1626280897608\n"
     ]
    }
   ],
   "source": [
    "print(a,b, id(a) ,id(b),sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 三种计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorFlow1.0时代，采用的是静态计算图，需要先使用TensorFlow的各种算子创建计算图，然后再开启一个会话Session，显式执行计算图\n",
    "TensorFlow2.0时代，采用的是动态计算图，即每使用一个算子后，该算子会被动态加入到隐含的默认计算图中立即执行得到结果，而无需开启Session。Eager Excution的好处是方便调试程序，它会让TensorFlow代码的表现和Python原生代码的表现一样，写起来就像写numpy一样，各种日志打印，控制流全部都是可以使用的。\n",
    "TensorFlow2.0中使用静态图，可以使用@tf.function装饰器将普通Python函数转换成对应的TensorFlow计算图构建代码。运行该函数就相当于在TensorFlow1.0中用Session执行代码。使用tf.function构建静态图的方式叫做 Autograph\n",
    "计算图由节点(nodes)和线(edges)组成。\n",
    "\n",
    "节点表示操作符Operator，或者称之为算子，线表示计算间的依赖。\n",
    "\n",
    "实线表示有数据传递依赖，传递的数据即张量。\n",
    "\n",
    "虚线通常可以表示控制依赖，即执行先后顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.1 静态计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello good'\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow 2.0  使用静态计算图方法\n",
    "g = tf.compat.v1.Graph()\n",
    "with g.as_default():\n",
    "    #placeholder为占位符，执行会话时候指定填充对象\n",
    "    x = tf.compat.v1.placeholder(name='x', shape=[], dtype=tf.string)\n",
    "    y = tf.compat.v1.placeholder(name='y', shape=[], dtype=tf.string)\n",
    "    z = tf.strings.join([x,y],name = \"join\",separator = \" \")\n",
    "\n",
    "with tf.compat.v1.Session(graph = g) as link:\n",
    "    # fetches的结果非常像一个函数的返回值，而feed_dict中的占位符相当于函数的参数序列。\n",
    "    result = link.run(fetches = z,feed_dict = {x:\"hello\",y:\"good\"})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在TensorFlow2.0中，使用的是动态计算图和Autograph，赋值之后即可立即使用\n",
    "x = tf.constant('hello')\n",
    "y = tf.constant('good')\n",
    "z = tf.strings.join([x,y], separator=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(b'hellogood', shape=(), dtype=string)\n",
      "hellogood\r\n"
     ]
    }
   ],
   "source": [
    "print(tf.rank(x))  # 对应之前，没有中括号，0维张量\n",
    "print(z)\n",
    "tf.print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hellogood\n",
      "tf.Tensor(b'hellogood', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#  封装成函数使用\n",
    "def joinstr(x,y):\n",
    "    z = tf.strings.join([x,y], separator=\"\")\n",
    "    tf.print(z)\n",
    "    return z\n",
    "\n",
    "ans = joinstr(x,y)   # x,y 之前已经定义，运行后可以直接使用\n",
    "print(ans)   # 函数化后效果类似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 实践中，我们一般会先用动态计算图调试代码，然后在需要提高性能的的地方利用@tf.function切换成Autograph获得更高的效率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function strjoin at 0x0000017AA5ED4708> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function strjoin at 0x0000017AA5ED4708>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function strjoin at 0x0000017AA5ED4708> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function strjoin at 0x0000017AA5ED4708>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "hello good\n",
      "tf.Tensor(b'hello good', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# @tf.function装饰器将普通Python函数转换成和TensorFlow1.0对应的静态计算图构建代码。\n",
    "\n",
    "@tf.function\n",
    "def strjoin(x,y):\n",
    "    z =  tf.strings.join([x,y],separator = \" \")\n",
    "    tf.print(z)\n",
    "    return z\n",
    "\n",
    "result = strjoin(x,y)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Trace already enabled\n",
      "WARNING:tensorflow:Entity <function strjoin at 0x0000017AA5ED4708> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function strjoin at 0x0000017AA5ED4708>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function strjoin at 0x0000017AA5ED4708> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function strjoin at 0x0000017AA5ED4708>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# 创建日志\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = './data/autograph/%s' % stamp\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "#开启autograph跟踪\n",
    "tf.summary.trace_on(graph=True, profiler=True) \n",
    "#执行autograph\n",
    "result = strjoin(\"hello\",\"world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autograph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-d63276dd0deb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"autograph\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         profiler_outdir=autograph)\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'autograph' is not defined"
     ]
    }
   ],
   "source": [
    "with writer.as_default():\n",
    "    tf.summary.trace_export(\n",
    "        name=\"autograph\",\n",
    "        step=0,\n",
    "        profiler_outdir=autograph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#启动tensorboard\n",
    "%tensorboard --logdir ./data/autograph/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 自动微分机制\n",
    "Tensorflow一般使用梯度磁带tf.GradientTape来记录正向运算过程，然后反播磁带自动得到梯度值。这种利用tf.GradientTape求微分的方法叫做Tensorflow的自动微分机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "tf.Tensor(-3.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# f(x) = a*x**2 + b*x + c的导数  a=1,b=-3,c=1\n",
    "x = tf.Variable(0.0, name='x', dtype=tf.float32)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-3.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    \n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-3.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# f(x) = a*x**2 + b*x + c的导数  a=1,b=-3,c=1\n",
    "# 对常量张量求导，增加watch\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([a,b,c])\n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    \n",
    "dy_dx,dy_da,dy_db,dy_dc = tape.gradient(y,[x,a,b,c])\n",
    "print(dy_dx,dy_da,dy_db,dy_dc, sep='\\n')   # 结果显示，x 默认为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "untimeError: GradientTape.gradient can only be called once on non-persistent tapes.\n",
    " GradientTape.gradient 每次使用时都要调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# f(x) = a*x**2 + b*x + c的导数  a=1,b=-3,c=1 求二阶导数\n",
    "with tf.GradientTape() as tape2:\n",
    "    with tf.GradientTape() as tape1:   \n",
    "        y = a*tf.pow(x,2) + b*x + c\n",
    "    dy_dx = tape1.gradient(y,x)   \n",
    "\n",
    "dy2_dx2 = tape2.gradient(dy_dx, x)\n",
    "print(dy2_dx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function f at 0x0000017AA57F5AF8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function f at 0x0000017AA57F5AF8>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function f at 0x0000017AA57F5AF8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function f at 0x0000017AA57F5AF8>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "(-2, 1)\n",
      "WARNING:tensorflow:Entity <function f at 0x0000017AA57F5AF8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function f at 0x0000017AA57F5AF8>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function f at 0x0000017AA57F5AF8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function f at 0x0000017AA57F5AF8>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "# 再 autograph 中使用\n",
    "@tf.function\n",
    "def f(x):\n",
    "    a, b, c = tf.constant(1.0), tf.constant(-2.0), tf.constant(1.0)\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        y = a*tf.pow(x,2) + b*x + c\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    return (dy_dx, y)\n",
    "\n",
    "tf.print(f(tf.constant(0.0)))\n",
    "tf.print(f(tf.Variable(1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.2 利用梯度磁带和优化器求最小值\n",
    "求f(x) = a*x**2 + b*x + c的最小值,使用optimizer.apply_gradients\n",
    "  实际函数参数赋值之后：y = x^2 - 2x +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=  0 ; x=  0.999998569\r\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-2.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)  # \n",
    "for _ in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = a*tf.pow(x,2) + b*x + c\n",
    "    dy_dx = tape.gradient(y,x)\n",
    "    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "\n",
    "tf.print(\"y= \", y, \"; x= \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0 ; x = 0.999999762\r\n"
     ]
    }
   ],
   "source": [
    "# 使用optimizer.minimize\n",
    "# optimizer.minimize相当于先用tape求gradient,再apply_gradient\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "\n",
    "#注意f()无参数\n",
    "def f():   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    y = a*tf.pow(x,2)+b*x+c\n",
    "    return(y)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)   # 学习率\n",
    "for _ in range(150):\n",
    "    optimizer.minimize(f,[x])   \n",
    "\n",
    "tf.print(\"y =\",f(),\"; x =\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用autograph 回溯版本报错，大致掌握以上两张即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
